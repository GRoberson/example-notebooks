{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc259945",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e8e57",
   "metadata": {},
   "source": [
    "# Vectara and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7548c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: langchain_community in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (0.0.20)\n",
      "Requirement already satisfied: langchain_openai in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (0.0.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain_openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.25.1)\n",
      "Requirement already satisfied: sniffio in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
      "Requirement already satisfied: httpcore in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a399ef-5cfb-4707-8ab1-8bbe1d68a169",
   "metadata": {},
   "source": [
    "[Vectara](https://vectara.com/) is a RAG-as-a-service platform for Retrieval Augmented Generation or RAG, which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store\n",
    "\n",
    "3. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments (including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/))\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "This notebook shows some examples of how to use Vectara with langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f945f-eff4-498a-974b-cf93f3202df6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need a Vectara account to use Vectara with LangChain. To get started, use the following steps:\n",
    "1. [Sign up](https://www.vectara.com/integrations/langchain) for a Vectara account if you don't already have one. Once you have completed your sign up you will have a Vectara customer ID. You can find your customer ID by clicking on your name, on the top-right of the Vectara console window.\n",
    "2. Within your account you can create one or more corpora. Each corpus represents an area that stores text data upon ingest from input documents. To create a corpus, use the **\"Create Corpus\"** button. You then provide a name to your corpus as well as a description. Optionally you can define filtering attributes and apply some advanced options. If you click on your created corpus, you can see its name and corpus ID right on the top.\n",
    "3. Next you'll need to create API keys to access the corpus. Click on the **\"Access Control\"** tab in the corpus view and then the **\"Create API Key\"** button. Give your key a name, and choose whether you want query only or query+index for your key. Click \"Create\" and you now have an active API key. Keep this key confidential. \n",
    "\n",
    "To use LangChain with Vectara, you'll need to provide your `customer ID`, `corpus ID` and an `api_key` to the LangChain Vectara class. You can do this in two ways:\n",
    "\n",
    "1. Include in your environment these three variables: `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`.\n",
    "\n",
    "> For example, you can set these variables using `os.environ` as follows (this points to one to a public Vectara corpus where the contents of vectara.com are indexed):\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key\n",
    "```\n",
    "\n",
    "2. Add them explicitly to the Vectara constructor:\n",
    "\n",
    "```python\n",
    "vectorstore = Vectara(\n",
    "                vectara_customer_id=customer_id,\n",
    "                vectara_corpus_id=corpus_id,\n",
    "                vectara_api_key=api_key\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794d8e-7c1c-4300-8a6d-e7665a5d3e5e",
   "metadata": {},
   "source": [
    "## Vectara: RAG-as-a-service\n",
    "\n",
    "Vectara is not a vector DB, it's much more than that - it is a full RAG-as-a-service platform. \n",
    "Yes, we have our own internal implementation of a scalable and serverless vector store, but that is just one piece of a whole set of components needed to implement RAG. The other components include text extraction, chunking, the Boomerang embedding model, advanced retrieval such as hybrid search or MMR, and more.\n",
    "\n",
    "You can ingest data into Vectara directly using Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing), using a tool like [vectara-ingest](https://github.com/vectara/vectara-ingest), or via the Vectara Langchain component directly. We will explore data ingest later in this notebook - for now let's assume you already ingested data into your Vectara corpus and see how querying works. \n",
    "\n",
    "We will utilize LangChain [LCEL](https://python.langchain.com/docs/expression_language/) which provides a nice syntax for chaining in LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c4d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66910837-637f-4682-9c35-925783e70beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vectara is an end-to-end platform that empowers product builders to embed powerful generative AI capabilities into applications. It offers significant improvements over traditional searches by understanding the context and meaning of data [1]. The platform enables developers to build a wide range of applications with powerful search experiences, without the risk of data or privacy violations [2]. Vectara provides a hybrid search approach that combines partial, exact, and Boolean text matching with neural models, allowing for more flexible and accurate retrieval of information [3]. It never trains on customer data, ensuring the security of user information [4]. Vectara's goal is to deliver contextually accurate responses and actions by deploying advanced zero-shot models and conversational search capabilities [4]. Overall, Vectara aims to revolutionize search technology and provide more accurate and insightful results to assist decision-making processes [1].\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Vectara\n",
    "\n",
    "# Instantiate the Vectara object, pointing it to the corpus as specified by the environment variables\n",
    "vectara = Vectara()\n",
    "\n",
    "# Define configuration for generative summary component and create the \"retriever\" object\n",
    "summary_config = {\n",
    "    \"is_enabled\": True, \"max_results\": 4, \n",
    "    \"response_lang\": \"en\",\n",
    "    \"prompt_name\": \"vectara-experimental-summary-ext-2023-10-23-small\"\n",
    "}\n",
    "retriever = vectara.as_retriever(\n",
    "    search_kwargs={ \"k\": 10, \"summary_config\": summary_config }\n",
    ")\n",
    "\n",
    "# The output of a query from Langchain is an array\n",
    "# The first K documents are the source documents, and entry K+1 is the summary\n",
    "# So we create convenience functions to grab those two pieces.\n",
    "def get_sources(documents):\n",
    "    return documents[:-1]\n",
    "def get_summary(documents):\n",
    "    return documents[-1].page_content\n",
    "\n",
    "# Let's run a query\n",
    "query_str = \"what is Vectara?\"\n",
    "(retriever | get_summary).invoke(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62735560-5502-4816-bd7d-0dd03f7e2ed8",
   "metadata": {},
   "source": [
    "Notice how simple the RAG pipeline is here. It does not require access to an OpenAI key or any other service for that matter, everything gets done inside the Vectara RAG platform. \n",
    "\n",
    "All you have to do is specify `summary_config`, with the following arguments:\n",
    "- `is_enabled`: True or False\n",
    "- `max_results`: number of results to use for summary generation\n",
    "- `response_lang`: language of the response summary, in ISO 639-2 format (e.g. 'en', 'fr', 'de', etc)\n",
    "\n",
    "This also allows us to take advantage of several query pre-processing capabilities that are part of Langchain: Self-query and multi-query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a8367-cb2f-45cb-8282-cfaceaaa92db",
   "metadata": {},
   "source": [
    "## Vectara Semantic Search\n",
    "\n",
    "You can also integrate Vectara just as a powerful semantic search engine. Similar to other `vector store`s in Langchain, in this case you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:\n",
    "\n",
    "```python\n",
    "results = vectara.similarity_score(\"what is Vectara?\")\n",
    "```\n",
    "The results are returned as a list of relevant documents, and a relevance score of each document.\n",
    "\n",
    "In this case, we used the default retrieval parameters, but you can also specify the following additional arguments in `similarity_search` or `similarity_search_with_score`:\n",
    "- `k`: number of results to return (defaults to 5)\n",
    "- `lambda_val`: the [lexical matching](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) factor for hybrid search (defaults to 0.025)\n",
    "- `filter`: a [filter](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview) to apply to the results (default None)\n",
    "- `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2.\n",
    "- `mmr_config`: can be used to specify MMR mode in the query.\n",
    "   - `is_enabled`: True or False\n",
    "   - `mmr_k`: number of results to use for MMR reranking\n",
    "   - `diversity_bias`: 0 = no diversity, 1 = full diversity. This is the lambda parameter in the MMR formula and is in the range 0...1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2218c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is the Vectara Platform? | Vectara Docs Welcome to the documentation homepage for Vectara , an end-to-end platform for product builders to embed powerful generative AI capabilities into applications with extraordinary results. Vectara offers significant improvements over traditional searches by understanding the context and meaning of your data. This revolutionary technology enables Vectara to drive insights and provide more accurate responses to user queries, assisting decision-making processes.', metadata={'lang': 'eng', 'offset': '0', 'len': '186', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs', 'title': 'What is the Vectara Platform? | Vectara Docs'}),\n",
       " Document(page_content=\"The Vectara Generative AI platform enables developers with the flexibility to build a wide range of applications with powerful search experiences. The Vectara platform never trains on customer data which enables businesses to embed generative AI capabilities without the risk of data or privacy violations. Vectara provides support for customer-managed keys, encryption at rest and during transit, client-configurable data retention, and more. If you're ready to dive into our APIs, make your way to our API Playground! This interactive environment allows you to experiment with Vectara 's REST APIs directly from your browser!\", metadata={'lang': 'eng', 'offset': '1831', 'len': '136', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs', 'title': 'What is the Vectara Platform? | Vectara Docs'}),\n",
       " Document(page_content='Hybrid Search: Combine Keyword and Semantic | Vectara Docs Vectara provides a Hybrid Search that offers a powerful and flexible approach to text retrieval. We combine partial, exact, and Boolean text matching with neural models which blends traditional, keyword-based search with semantic search in what is called \"hybrid\" retrieval model. For example, Vectara enables you to do the following:\\n• Include exact keyword matches for occasions where a search term was absent from Vectara\\'s training data (e.g. product SKUs)\\n• Disable neural retrieval entirely, and instead use exact term matching\\n• Incorporate typical keyword modifiers like a function, exact phrase matching, and wildcard prefixes of terms\\n\\nThe exact and Boolean text matching (similar to a traditional, keyword-based search) is disabled by default and Vectara only uses neural retrieval.', metadata={'lang': 'eng', 'offset': '0', 'len': '96', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/semantic-search/hybrid-search', 'title': 'Hybrid Search: Combine Keyword and Semantic | Vectara Docs'}),\n",
       " Document(page_content='User data remains secure because Vectara never trains on customer data. The Vectara team envisions a future where generative AI powers every application to deliver contextually accurate responses and give the right answers and actions. Vectara is built on a solid hybrid search core to enable better generative outcomes. Traditional search technologies focus on keywords, which limit their ability to understand complex queries. Vectara deploys advanced zero-shot models and conversational search capabilities to understand, interpret, and respond to user queries with remarkable precision.', metadata={'lang': 'eng', 'offset': '699', 'len': '84', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs', 'title': 'What is the Vectara Platform? | Vectara Docs'}),\n",
       " Document(page_content='Using Vectara is like having a global research assistant that can read and understand large volumes of documents in an instant. Let the platform speed up your research process, find the most relevant information, and become a recommendation system for your domain. Vectara can help transform data into insights which help make decision-making easier. This platform can provide hidden insights and patterns from your data, helping you make informed decisions. Not only can it answer your questions, but also provides citations grounded in facts from the raw data.', metadata={'lang': 'eng', 'offset': '4052', 'len': '85', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/use-case-exploration', 'title': 'Explore the Vectara Use Cases | Vectara Docs'}),\n",
       " Document(page_content='Enable your users to ask a question and get the precise answers quickly. Embed your FAQs, customer support interactions, product manuals, inform knowledge workers on data, and enhance your website search. Vectara empowers your organization to create a dynamic, responsive, and continuous improving Question and Answer system that enhances the user experience and provides context-aware answers. Vectara sifts through volumes of publications, news articles, financial reports, scientific and medical research, corporate documents and more and provides summarized answers to guide decision-making in your domain. Collaborate with researchers to streamline the peer review process by investigating topics and questions in these vast volumes of data to identify key insights.', metadata={'lang': 'eng', 'offset': '3217', 'len': '189', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/use-case-exploration', 'title': 'Explore the Vectara Use Cases | Vectara Docs'}),\n",
       " Document(page_content='Traditional search technologies focus on keywords, which limit their ability to understand complex queries. Vectara deploys advanced zero-shot models and conversational search capabilities to understand, interpret, and respond to user queries with remarkable precision. Vectara summarizes search results on complex queries while providing factual citations from your data. Vectara provides the best hybrid search core and superior language understanding for ingestion and retrieval. Vectara can become your answer engine.', metadata={'lang': 'eng', 'offset': '1055', 'len': '102', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs', 'title': 'What is the Vectara Platform? | Vectara Docs'}),\n",
       " Document(page_content='Vectara deploys advanced zero-shot models and conversational search capabilities to understand, interpret, and respond to user queries with remarkable precision. Vectara summarizes search results on complex queries while providing factual citations from your data. Vectara provides the best hybrid search core and superior language understanding for ingestion and retrieval. Vectara can become your answer engine. Designed for developers with an API-first approach, Vectara is the optimal choice to integrate generative AI search into your applications.', metadata={'lang': 'eng', 'offset': '1158', 'len': '109', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs', 'title': 'What is the Vectara Platform? | Vectara Docs'}),\n",
       " Document(page_content='You provide data and queries through simple APIs in our SaaS service. Enable your users to find the most relevant products, support cases, and documents that answer their questions. Vectara helps you bridge the gap between user queries and the vast amount of data within an application. Users can find exactly what they are looking for despite how they ask.', metadata={'lang': 'eng', 'offset': '4632', 'len': '104', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/use-case-exploration', 'title': 'Explore the Vectara Use Cases | Vectara Docs'}),\n",
       " Document(page_content=\"Recommendation System | Vectara Docs Vectara can be used as a semantic recommendation system out of the box in order to provide your users with semantically similar documents/products. Before you begin using Vectara for a semantic recommendation system, it's useful to think through what types of recommendation flows you want to enable. For example:\\n• Do you want to recommend based on the entire document content or just 1 section/field like the document title?\", metadata={'lang': 'eng', 'offset': '0', 'len': '147', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/recommendation-systems/recommender-overview', 'title': 'Recommendation System | Vectara Docs'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_str = \"what is Vectara?\"\n",
    "(retriever | get_sources).invoke(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f355b",
   "metadata": {},
   "source": [
    "## Using LangChain's MultiQueryRetriever with Vectara\n",
    "\n",
    "One of the great features of LangChain is the availability of advanced retreivers such as the MultiQuery retreiver.\n",
    "\n",
    "The MultiQueryRetriever uses an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. \n",
    "\n",
    "Let's see how to use MultiQuery Retreiver with Vectara. In this case, you do need to use OpenAI directly from langchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999fce1c-1647-4f5b-8c32-a1741950287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with regular query = Vectara supports hybrid search, combining keyword-based search with semantic search in a single platform [3]. It offers a flexible approach to text retrieval, blending traditional, keyword-based search with neural models [2]. By default, Vectara uses semantic similarity, but it can introduce keyword-focused algorithms to improve relevance [4]. This platform enables users to embed powerful hybrid search into their applications through simple APIs [1]. Vectara's hybrid search allows for exact keyword matches, disabling neural retrieval, and incorporating keyword modifiers [3]. It aims to deliver contextually accurate responses and generate better outcomes [2].\n",
      "\n",
      "Response with MQ = The Vectara platform integrates hybrid search with MMR functionality in one solution. It combines partial, exact, and Boolean text matching with neural models, enabling a powerful and flexible approach to text retrieval [3]. Vectara deploys advanced zero-shot models and conversational search capabilities to understand and respond to user queries accurately [2]. It provides a secure environment for user data and offers an API-first approach for developers to integrate generative AI search into their applications [4]. Vectara's hybrid search core and superior language understanding make it an optimal choice for embedding powerful search capabilities [1].\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "query_str = \"Does Vectara support hybrid search and MMR in a single platform?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo-preview\")\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "resp1 = (retriever | get_summary).invoke(query_str)\n",
    "resp2 = (mqr | get_summary).invoke(query_str)\n",
    "\n",
    "print(f\"Response with regular query = {resp1}\\n\")\n",
    "\n",
    "print(f\"Response with MQ = {resp2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ff77f",
   "metadata": {},
   "source": [
    "## SelfQuery Retreiver\n",
    "\n",
    "Another such query pre-processing capability to mention here is the `SelfQueryRetriever`, where a user query can be transformed into a sub-query with a set of filtering conditions. \n",
    "\n",
    "For example in an e-commerce dataset a query like \"which products cost at least 100 dollars and are blue\" might be converted into \"blue products\" with a filtering condition of \"doc.price > 100\" (assuming \"price\" is a meta-data field).\n",
    "\n",
    "You can find a complete example for using the `SelfQUeryRetriever` [here](https://python.langchain.com/docs/integrations/retrievers/self_query/vectara_self_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee237e",
   "metadata": {},
   "source": [
    "## LangChain Vectara Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac4600",
   "metadata": {},
   "source": [
    "LangChain templates offer a collection of easily deployable reference architectures, and there are two templates for using Vectara:\n",
    "* [RAG](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara) template for basic RAG.\n",
    "* [RAG with multi-query](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara-multiquery) for using Vectara RAG with the multi-query retriever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c999c-930d-4a46-b397-7c32dad08d5f",
   "metadata": {},
   "source": [
    "## Data Ingestion with LangChain\n",
    "\n",
    "Even though it is more common to use Vectara with LangChain for query purposes, it is also possible to ingest data into Vectara via LangChain. There are two main functions that are useful for this purpose: `add_texts` (or `add_documents` which is similar with a lightly different interface) and `add_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfba86-8bed-4394-bbd1-667ef38830f4",
   "metadata": {},
   "source": [
    "For `add_texts` the input is simply a set of text strings:\n",
    "\n",
    "```python\n",
    "vectara.add_texts([\"to be or not to be\", \"that is the question\"])\n",
    "```\n",
    "\n",
    "A common pattern is to use one of LangChain's data upload classes, extract the text from there, and then upload the text. Note that no chunking is necessary (although it is optional) in this case since Vectara performs its own optimal chunking.\n",
    "\n",
    "Since Vectara supports [file upload](https://docs.vectara.com/docs/api-reference/indexing-apis/file-upload/file-upload) natively, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly in the LangChain class. When using this method, the file is uploaded directly to the Vectara platform, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism.\n",
    "\n",
    "As an example:\n",
    "\n",
    "```python\n",
    "vectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
