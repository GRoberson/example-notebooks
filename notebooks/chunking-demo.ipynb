{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce806fa1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/chunking-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, Vectara\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54a7d7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f73ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/llama2.pdf', <http.client.HTTPMessage at 0x133168e20>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://arxiv.org/pdf/2307.09288.pdf'\n",
    "file_name = '../data/llama2.pdf'\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c9ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo-16k', temperature=0)\n",
    "\n",
    "def get_answer(doc_text, chunk_size: int, chunk_overlap: int, query: str):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(doc_text)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vs = Chroma.from_documents(docs, embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vs.as_retriever())\n",
    "    return qa.run(query)\n",
    "\n",
    "def get_answer_recursive(doc_text, chunk_size: int, chunk_overlap: int, query: str):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(doc_text)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vs = Chroma.from_documents(docs, embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vs.as_retriever())\n",
    "    return qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a938020",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(file_name, mode=\"single\", strategy=\"fast\")\n",
    "doc_text = loader.load()\n",
    "\n",
    "query1 = \"what is shown in figure 16?\"\n",
    "query2 = \"is GPT-4 better than Llama2?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bf82f",
   "metadata": {},
   "source": [
    "# Test fixed-size chunking with LangChain for query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c14b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1552, which is longer than the specified 1000\n",
      "Created a chunk of size 1223, which is longer than the specified 1000\n",
      "Created a chunk of size 1385, which is longer than the specified 1000\n",
      "Created a chunk of size 1844, which is longer than the specified 1000\n",
      "Created a chunk of size 1159, which is longer than the specified 1000\n",
      "Created a chunk of size 1557, which is longer than the specified 1000\n",
      "Created a chunk of size 2274, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1658, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1277, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1148, which is longer than the specified 1000\n",
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n",
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1552, which is longer than the specified 1000\n",
      "Created a chunk of size 1223, which is longer than the specified 1000\n",
      "Created a chunk of size 1385, which is longer than the specified 1000\n",
      "Created a chunk of size 1844, which is longer than the specified 1000\n",
      "Created a chunk of size 1159, which is longer than the specified 1000\n",
      "Created a chunk of size 1557, which is longer than the specified 1000\n",
      "Created a chunk of size 2274, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1658, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1277, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1148, which is longer than the specified 1000\n",
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=0, response=I'm sorry, but I don't have access to any figures or images.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2274, which is longer than the specified 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=100, response=I'm sorry, but I don't have access to any figures or images.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2274, which is longer than the specified 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=2000, overlap=0, response=I'm sorry, but I don't have any information about Figure 16.\n",
      "\n",
      "chunk=2000, overlap=100, response=I'm sorry, but I don't have any information about Figure 16.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = query1\n",
    "\n",
    "for chunk_size in [1000, 2000]:\n",
    "    for chunk_overlap in [0, 100]:\n",
    "        response = get_answer(doc_text, chunk_size, chunk_overlap, query)\n",
    "        print(f\"chunk={chunk_size}, overlap={chunk_overlap}, response={response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff05b0d",
   "metadata": {},
   "source": [
    "# Test Recursive chunking with LangChain for query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e098a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=0, response=I'm sorry, but I don't have any information about Figure 16.\n",
      "\n",
      "chunk=1000, overlap=100, response=I'm sorry, but I don't have access to any figures or images.\n",
      "\n",
      "chunk=2000, overlap=0, response=I'm sorry, but I don't have access to any figures or images.\n",
      "\n",
      "chunk=2000, overlap=100, response=The context does not provide any information about what is shown in Figure 16.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in [1000, 2000]:\n",
    "    for chunk_overlap in [0, 100]:\n",
    "        response = get_answer_recursive(doc_text, chunk_size, chunk_overlap, query)\n",
    "        print(f\"chunk={chunk_size}, overlap={chunk_overlap}, response={response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096aa41",
   "metadata": {},
   "source": [
    "# Test Vectara with LangChain for query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483fd55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 16 shows the distribution of safety RM scores from the base model, when adding a generic preprompt, and when adding a preprompt based on the risk category with a tailored answer template. It illustrates how the addition of a preprompt with a tailored answer template helps increase safety RM scores even more compared to a generic preprompt.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = Vectara.from_documents(doc_text, embedding=None)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vs.as_retriever())\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d32bf",
   "metadata": {},
   "source": [
    "# Test fixed-size chunking with LangChain for query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8f936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1552, which is longer than the specified 1000\n",
      "Created a chunk of size 1223, which is longer than the specified 1000\n",
      "Created a chunk of size 1385, which is longer than the specified 1000\n",
      "Created a chunk of size 1844, which is longer than the specified 1000\n",
      "Created a chunk of size 1159, which is longer than the specified 1000\n",
      "Created a chunk of size 1557, which is longer than the specified 1000\n",
      "Created a chunk of size 2274, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1658, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1277, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1148, which is longer than the specified 1000\n",
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n",
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1552, which is longer than the specified 1000\n",
      "Created a chunk of size 1223, which is longer than the specified 1000\n",
      "Created a chunk of size 1385, which is longer than the specified 1000\n",
      "Created a chunk of size 1844, which is longer than the specified 1000\n",
      "Created a chunk of size 1159, which is longer than the specified 1000\n",
      "Created a chunk of size 1557, which is longer than the specified 1000\n",
      "Created a chunk of size 2274, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1545, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1658, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1277, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1148, which is longer than the specified 1000\n",
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=0, response=Yes, according to the provided information, there is still a large gap in performance between Llama 2 70B and GPT-4.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2274, which is longer than the specified 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=100, response=Yes, according to the provided information, there is still a large gap in performance between Llama 2 70B and GPT-4.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2274, which is longer than the specified 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=2000, overlap=0, response=Yes, according to the provided information, there is still a large gap in performance between Llama 2 70B and GPT-4.\n",
      "\n",
      "chunk=2000, overlap=100, response=Yes, according to the provided information, there is still a large gap in performance between Llama 2 70B and GPT-4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = query2\n",
    "\n",
    "for chunk_size in [1000, 2000]:\n",
    "    for chunk_overlap in [0, 100]:\n",
    "        response = get_answer(doc_text, chunk_size, chunk_overlap, query)\n",
    "        print(f\"chunk={chunk_size}, overlap={chunk_overlap}, response={response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c35020",
   "metadata": {},
   "source": [
    "# Test Recursive chunking with LangChain for query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b847c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=1000, overlap=0, response=Based on the provided information, it is not explicitly stated whether GPT-4 is better than Llama2. However, it is mentioned that GPT-4 performs better than other non-Meta reward models, but there is still a large gap in performance between Llama 2 70B and GPT-4. Therefore, it is unclear if GPT-4 is better than Llama2.\n",
      "\n",
      "chunk=1000, overlap=100, response=Based on the provided information, it is not explicitly stated whether GPT-4 is better than Llama2. However, it is mentioned that Llama 2 70B is close to GPT-3.5 on some benchmarks but there is a significant gap on coding benchmarks. It is also mentioned that there is still a large gap in performance between Llama 2 70B and GPT-4. Therefore, it can be inferred that GPT-4 may have better performance than Llama2, but without specific benchmark results, we cannot make a definitive conclusion.\n",
      "\n",
      "chunk=2000, overlap=0, response=Based on the provided information, it is not explicitly stated whether GPT-4 is better than Llama2. However, it is mentioned that GPT-4 performs better than other non-Meta reward models, but there is still a large gap in performance between Llama 2 70B and GPT-4. Therefore, it is unclear if GPT-4 is better than Llama2.\n",
      "\n",
      "chunk=2000, overlap=100, response=Based on the provided information, it is not explicitly stated whether GPT-4 is better than Llama2. However, it is mentioned that GPT-4 performs better than other non-Meta reward models, but there is still a large gap in performance between Llama 2 70B and GPT-4. Therefore, it is unclear if GPT-4 is better than Llama2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in [1000, 2000]:\n",
    "    for chunk_overlap in [0, 100]:\n",
    "        response = get_answer_recursive(doc_text, chunk_size, chunk_overlap, query)\n",
    "        print(f\"chunk={chunk_size}, overlap={chunk_overlap}, response={response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee85db",
   "metadata": {},
   "source": [
    "# Test Vectara with LangChain for query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8562fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, it is stated that there is still a large gap in performance between Llama 2 70B and GPT-4. Therefore, it can be inferred that GPT-4 is considered to be better than Llama 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = Vectara.from_documents(doc_text, embedding=None)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vs.as_retriever())\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
