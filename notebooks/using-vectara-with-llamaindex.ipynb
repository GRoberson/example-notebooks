{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted GenAI and semantic search platform that provides an easy-to-use API for document indexing and querying. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedder model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments (including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/))\n",
    "\n",
    "4. An option to create [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits for using Vectara for a RAG application are:\n",
    "* **Easy to use**: Vectara takes care of much detail required for a fully functional, highly scalable and robust RAG application, so as a user you don't have to code up these pieces and maintain them over time\n",
    "* **Scalable and Secure**: building GenAI applications may seem easy at first, and Vectara provides instant scalablility to millions of documents, while maintaing data security and privacy, as well as latency SLAs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About Llama Index\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙.\n",
    "\n",
    "To get started with Vectara, [sign up](https://vectara.com/integrations/llamaindex) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (0.9.36)\n",
      "Requirement already satisfied: arxiv in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (2023.12.2)\n",
      "Requirement already satisfied: httpx in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (0.25.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (1.7.1)\n",
      "Requirement already satisfied: pandas in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests>=2.31.0->llama-index) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests>=2.31.0->llama-index) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from requests>=2.31.0->llama-index) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: httpcore in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from httpx->llama-index) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from dataclasses-json->llama-index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ofer/miniconda3/envs/langchain/lib/python3.10/site-packages (from httpcore->httpx->llama-index) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-index arxiv\n",
    "\n",
    "import os\n",
    "#os.environ['VECTARA_API_KEY'] = '<VECTARA_API_KEY>'\n",
    "#os.environ['VECTARA_CORPUS_ID'] = '<VECTARA_CORPUS_ID>'\n",
    "#os.environ['VECTARA_CUSTOMER_ID'] = '<VECTARA_CURSTOMER_ID>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We would pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3',\n",
       " 'http://arxiv.org/abs/2105.04339v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index import Document\n",
    "from llama_index.indices import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content ofr all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.entry_id,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }        \n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (where author is the first author if there are many, just to simplify). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the VectaraIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding refers to the process of representing sentences as fixed-length numerical vectors. It involves encoding sentences into vector representations using techniques like LSTM encoders [3] and then using these embeddings for downstream natural language processing (NLP) tasks [2]. Sentence embedding models can be used for tasks such as attribute inference [1] and softmax inference classification [4]. These models aim to capture the semantic meaning and contextual information of sentences, enabling various NLP applications.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "When we use `print(response)` it simply prints the response text. But the `response` object also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/abs/2305.15077v2'),\n",
       " (1, 'http://arxiv.org/abs/2305.03010v1'),\n",
       " (2, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (3, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/abs/1904.05542v1')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx,n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    "Vectara supports [max-marginal-relevance](https://docs.vectara.com/docs/api-reference/search-apis/reranking#maximal-marginal-relevance-mmr-reranker) natively in the backend, and this is available as a query mode. \n",
    "\n",
    "Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where mmr_diversity_bias=0.3 provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding refers to the representation of sentences in a vector or matrix form, capturing their semantic meaning and contextual information [1]. Various methods have been proposed for sentence embedding, including the use of recurrent neural networks, self-attention mechanisms, and deep contextualized word models [2][3][4]. These techniques aim to encode different parts of the sentence and visualize the encoded information [2]. Evaluations have shown that sentence embedding models outperform other methods in tasks such as author profiling, sentiment classification, and textual entailment [2]. However, the effectiveness of preserving the original sentence meanings in embedded vectors is still an ongoing research area [4]. Analogical relationships and regularities can also be explored using sentence embedding spaces [6]. One approach, called Relational Sentence Embedding (RSE), leverages multi-source relational data for improved generalizability [7]. Overall, sentence embedding plays a crucial role in natural language processing, enabling better understanding and analysis of textual data [4].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    vectara_query_mode=\"mmr\",\n",
    "    mmr_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/abs/1703.03130v1'),\n",
       " (1, 'http://arxiv.org/abs/2305.15077v2'),\n",
       " (2, 'http://arxiv.org/abs/2002.09620v2'),\n",
       " (3, 'http://arxiv.org/abs/1808.05505v3'),\n",
       " (4, 'http://arxiv.org/abs/1904.05542v1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx,n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (like OpenAI's GPT-4 or similar) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this you would need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding refers to the process of representing a sentence as a fixed-length vector in a high-dimensional space. This vector representation captures the semantic meaning and contextual information of the sentence, allowing for various natural language processing tasks such as text classification, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43963c4",
   "metadata": {},
   "source": [
    "## Using Auto Retriever with Vectara\n",
    "\n",
    "LlamaIndex's auto-retriever functionality is really cool. \n",
    "It is most useful when you have metadata fields (like in our case of papers from Arxiv), and would like a query that references a metadata field to be automatically interpreted in the right way.\n",
    "\n",
    "For example, if I ask \"what is a paper about climate change risks published after 2020\", the auto-retriever would (behind the scences) interpret ths into a query \"what is a paper about climate change risks\" along with a filter condition of \"published > 2020\"\n",
    "\n",
    "Let's see how this works with the Vectara Index.\n",
    "First, we have to define a `VectorStoreInfo` structure that defines the meta data fields the auto-retriever knows about to do its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"information about a paper\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"published\",\n",
    "            description=\"The date the paper was published\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            description=\"The author of the paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The title of the papers\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"url\",\n",
    "            description=\"The URL for this paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23712d46",
   "metadata": {},
   "source": [
    "Auto-retrieval is implemented before calling Vectara as a query transformation. This step uses an LLM, so we need to define access to that LLM. Let's do that here and use the new GPT4-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424fa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "llm=OpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f0333",
   "metadata": {},
   "source": [
    "Now we can define the `VectaraAutoRetriever`, which can perform auto-retrieval using Vectara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92de30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding\n",
      "Using implicit filters: [('published', '<', 2019)]\n",
      "final filter string: (doc.published < '2019')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2017-03-09',\n",
       "  'Instead of using a vector, we use a 2-D matrix\\nto represent the embedding, with each row of the matrix attending on a different\\npart of the sentence. We also propose a self-attention mechanism and a special\\nregularization term for the model. As a side effect, the embedding comes with an\\neasy way of visualizing what speciﬁc parts of the sentence are encoded into the\\nembedding. We evaluate our model on 3 different tasks: author proﬁling, senti-\\nment classiﬁcation and textual entailment. Results show that our model yields a\\nsigniﬁcant performance gain compared to other sentence embedding methods in\\nall of the 3 tasks.'),\n",
       " ('2018-08-16',\n",
       "  'This\\nproblem can be alleviated by obtaining more of para-\\nphrase sentence pairs. Conclusion Sentence embedding is one of the most important text\\nprocessing techniques in NLP. To date,  various sen-\\ntence embedding models have been proposed and have\\nyielded good performances in document classification\\nand sentiment analysis tasks. However, the fundamen-\\ntal ability of sentence embedding methods, i.e., how\\neffectively the meanings of the original sentences are\\npreserved  in  the  embedded  vectors,  cannot  be  fully\\nevaluated through such indirect methods.'),\n",
       " ('2018-08-16',\n",
       "  'Paraphrase Thought:  Sentence Embedding Module Imitating\\n                        Human Language Recognition Myeongjun Jang 1 Abstract\\nSentence embedding is an important research\\ntopic in natural language processing. It is es-\\nsential to generate a good embedding vector\\nthat  fully  reflects  the  semantic  meaning  of\\na sentence in order to achieve an enhanced\\nperformance  for  various  natural  language\\nprocessing  tasks,   such  as  machine  trans-\\nlation  and  document  classification. Thus\\nfar, various sentence embedding models have\\nbeen proposed, and their feasibility has been\\ndemonstrated through good performances on\\ntasks following embedding, such as sentiment\\nanalysis  and  sentence  classification.'),\n",
       " ('2018-08-16',\n",
       "  'Pilsung Kang 1 Introduction Sentence embedding, which transforms sentences into\\nlow-dimensional  vector  values  reflecting  their  mean-\\nings,  is a highly important task in natural language\\nprocessing  (NLP). By  mapping  unstructured  text\\ndata into a certain form of structured representation,\\nthe embedding vector can enhance the performances\\nof  various  NLP  tasks,  such  as  machine  translation\\n(Artetxe et al., 2017; Lee et al., 2016; Zhao & Zhang,\\n2016), document classification (Conneau et al., 2017b;\\nZhou et al., 2016), and sentence matching (Wan et al.,\\n2016). As sentence embedding plays an import role\\nin NLP, various methods (Kiros et al., 2015; Pagliar-\\ndini et al., 2017; Hill et al., 2016; Arora et al., 2017;\\nConneau  et  al.,  2017a;  Chen,  2017)  have  been  pro-\\nposed since the advent of the Doc2vec method (Le &\\nMikolov, 2014).'),\n",
       " ('2016-06-15',\n",
       "  '2    Siamese CBOW\\n\\nWe present the Siamese Continuous Bag of Words\\n(CBOW)  model,  a  neural  network  for  efﬁcient\\nestimation of high-quality sentence embeddings. Quality should manifest itself in embeddings of\\nsemantically close sentences being similar to one\\nanother, and embeddings of semantically different\\nsentences being dissimilar. An efﬁcient and sur-\\nprisingly successful way of computing a sentence\\nembedding is to average the embeddings of its\\nconstituent words. Recent work uses pre-trained\\nword embeddings (such as word2vec and GloVe)\\nfor this task, which are not optimized for sentence\\nrepresentations. Following these approaches, we\\ncompute sentence embeddings by averaging word\\nembeddings, but we optimize word embeddings\\ndirectly for the purpose of being averaged.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "retriever = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    service_context=service_context,\n",
    "    verbose=True\n",
    ")\n",
    "res = retriever.retrieve(\"What is sentence embedding, based on papers before 2019?\")\n",
    "[(r.metadata['published'], r.text) for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7450bc",
   "metadata": {},
   "source": [
    "As you can see, the Auto Retriever was able to translate the natural language text into a shorter query and a proper condition (in this case `doc.published < 2019`).\n",
    "\n",
    "We can also of course ask a question directly: we use the `VectaraQueryEngine` which can work with the `VectaraAutoRetriever` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n",
      "Sentence embedding is a crucial technique in natural language processing (NLP) that aims to represent sentences as low-dimensional vectors, capturing their semantic meaning [1]. Various models have been proposed to generate effective sentence embeddings, such as using a 2-D matrix to represent the embedding and incorporating self-attention mechanisms [1]. Evaluations have shown that these models outperform other methods in tasks like author profiling, sentiment classification, and textual entailment [1]. The quality of sentence embeddings is determined by how well they preserve the original sentence meanings [3]. Sentence embedding plays a vital role in NLP tasks like machine translation, document classification, and sentence matching [4]. The Siamese Continuous Bag of Words (CBOW) model is a neural network that efficiently estimates high-quality sentence embeddings by averaging word embeddings [5].\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara.query import VectaraQueryEngine\n",
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "ar = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    service_context=service_context,\n",
    "    summary_enabled=True,\n",
    "    summary_num_results=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query_engine = VectaraQueryEngine(retriever=ar)\n",
    "response = query_engine.query(\"What is sentence embedding, based on papers before 2019?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5faef-9fe9-4acc-a55d-ad7c379697fd",
   "metadata": {},
   "source": [
    "## Advanced querying with QueryFusionRetriever\n",
    "\n",
    "The QueryFusion [Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html#reciprocal-rerank-fusion-retriever) is an advanced query mechanism whereby the original query is pre-processed to generate N variations. Each of these rephrased queries is then run against the Vectara engine and rank-fusion is used to combine the best results. \n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c662bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT is not a dual encoder. It uses a different architecture, although the specific architecture is not mentioned in the given context information.\n"
     ]
    }
   ],
   "source": [
    "query = \"is SBERT a dual encoder? if not, what architecture does it use?\"\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    summary_enabled=False,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896e8b7e-c028-4ec6-bb7c-0c05dfb86321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is the architecture of SBERT?\n",
      "2. How does SBERT work as a sentence embedding model?\n",
      "3. Explain the encoder used in SBERT.\n",
      "4. Compare SBERT with other dual encoder models.\n",
      "SBERT is not a dual encoder. The architecture used by SBERT is based on the Sentence Transformer framework.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers import QueryFusionRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "import nest_asyncio\n",
    "\n",
    "rf_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(similarity_top_k=2)],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=5,  # this includes the origianl query; set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "nest_asyncio.apply()     # apply nested async to run in a notebook\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(rf_retriever)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6b3fd",
   "metadata": {},
   "source": [
    "We can see how the QueryFusionRetriever created additional query variations (they are displayed since we used `verbose=True`) and then the overall response includes the results fused together. This is very helpful in this case because the QueryFusionRetriever creates sub-questions that inquire about the specific architecture of SBERT which is relevant context to answering this question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185e43c",
   "metadata": {},
   "source": [
    "## The Vectara-RAG LlamaPack\n",
    "\n",
    "[Llama Packs](https://docs.llamaindex.ai/en/stable/community/llama_packs/root.html) are a community-driven hub of prepackaged modules for LlamaIndex.\n",
    "\n",
    "Vectara's integration with LlamaIndex provides Vectara RAG, a Llama Pack with ready-to-go RAG.\n",
    "\n",
    "Try it out for yourself by following these [instructions](https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs/vectara_rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and calls the LLM for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the tools provided by LlamaIndex like Auto Retrieval and Query Fusion, you can easily build and test advanced RAG applications at enteprise scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c950620-9618-4d2c-9597-58ec5adae393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
